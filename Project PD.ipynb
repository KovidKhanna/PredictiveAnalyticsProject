{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc86f0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip install python-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb10ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124800, 784) (124800,)\n"
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "\n",
    "data = MNIST(path=r'C:\\Users\\darkv\\Predictive analysis project\\data', return_type='numpy')\n",
    "data.select_emnist('letters')\n",
    "\n",
    "X, y = data.load_training()\n",
    " \n",
    "print(X.shape, y.shape) # 28*28\n",
    " \n",
    "X = X.reshape(124800, 28, 28)\n",
    "y = y.reshape(124800, 1)\n",
    " \n",
    "y = y-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07807522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=50)\n",
    " \n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    " \n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes = 26)\n",
    "y_test = to_categorical(y_test, num_classes = 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66abbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darkv\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,338</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             │        \u001b[38;5;34m13,338\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">677,914</span> (2.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m677,914\u001b[0m (2.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">677,914</span> (2.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m677,914\u001b[0m (2.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 3. Define our model\n",
    " \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (28,28)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2)) # preventing overfitting\n",
    "model.add(Dense(512, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(26, activation='softmax'))\n",
    " \n",
    "model.summary()\n",
    " \n",
    "model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6216345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m614/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6431 - loss: 1.2126\n",
      "Epoch 1: val_loss improved from inf to 0.44047, saving model to best_model.keras\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.6451 - loss: 1.2054 - val_accuracy: 0.8627 - val_loss: 0.4405\n",
      "Epoch 2/10\n",
      "\u001b[1m616/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8600 - loss: 0.4451\n",
      "Epoch 2: val_loss improved from 0.44047 to 0.34808, saving model to best_model.keras\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8601 - loss: 0.4448 - val_accuracy: 0.8904 - val_loss: 0.3481\n",
      "Epoch 3/10\n",
      "\u001b[1m622/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8893 - loss: 0.3428\n",
      "Epoch 3: val_loss improved from 0.34808 to 0.31878, saving model to best_model.keras\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8893 - loss: 0.3428 - val_accuracy: 0.8960 - val_loss: 0.3188\n",
      "Epoch 4/10\n",
      "\u001b[1m621/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9013 - loss: 0.2930\n",
      "Epoch 4: val_loss improved from 0.31878 to 0.29523, saving model to best_model.keras\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9013 - loss: 0.2930 - val_accuracy: 0.9043 - val_loss: 0.2952\n",
      "Epoch 5/10\n",
      "\u001b[1m615/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9108 - loss: 0.2603\n",
      "Epoch 5: val_loss improved from 0.29523 to 0.29035, saving model to best_model.keras\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9108 - loss: 0.2604 - val_accuracy: 0.9060 - val_loss: 0.2903\n",
      "Epoch 6/10\n",
      "\u001b[1m619/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9173 - loss: 0.2346\n",
      "Epoch 6: val_loss improved from 0.29035 to 0.28375, saving model to best_model.keras\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9173 - loss: 0.2347 - val_accuracy: 0.9086 - val_loss: 0.2838\n",
      "Epoch 7/10\n",
      "\u001b[1m620/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9244 - loss: 0.2163\n",
      "Epoch 7: val_loss improved from 0.28375 to 0.28059, saving model to best_model.keras\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9244 - loss: 0.2164 - val_accuracy: 0.9121 - val_loss: 0.2806\n",
      "Epoch 8/10\n",
      "\u001b[1m619/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9280 - loss: 0.1985\n",
      "Epoch 8: val_loss did not improve from 0.28059\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9280 - loss: 0.1986 - val_accuracy: 0.9109 - val_loss: 0.2817\n",
      "Epoch 9/10\n",
      "\u001b[1m619/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9320 - loss: 0.1891\n",
      "Epoch 9: val_loss improved from 0.28059 to 0.28025, saving model to best_model.keras\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9320 - loss: 0.1891 - val_accuracy: 0.9142 - val_loss: 0.2802\n",
      "Epoch 10/10\n",
      "\u001b[1m619/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9358 - loss: 0.1757\n",
      "Epoch 10: val_loss did not improve from 0.28025\n",
      "\u001b[1m624/624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9358 - loss: 0.1758 - val_accuracy: 0.9161 - val_loss: 0.2829\n",
      "Test accuracy is  91.54247045516968\n"
     ]
    }
   ],
   "source": [
    "### 4. calculate accuracy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    " \n",
    "checkpointer = ModelCheckpoint(filepath='best_model.keras', verbose=1, save_best_only=True)\n",
    "model.fit(X_train, y_train, batch_size = 128, epochs= 10, validation_split = 0.2, \n",
    "          callbacks=[checkpointer], verbose=1, shuffle=True)\n",
    " \n",
    "model.load_weights('best_model.keras')\n",
    " \n",
    "# calculate test accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    " \n",
    "print(\"Test accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "879ce0d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f60d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35cc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters ={ 0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:'f', 6:'g', 7:'h', 8:'i', 9:'j', 10:'k', 11:'l', \n",
    "          12:'m', 13:'n', 14:'o', 15:'p', 16:'q', 17:'r', 18:'s', 19:'t', 20:'u', 21:'v', 22:'w', \n",
    "          23:'x', 24:'y', 25:'z', 26:''}\n",
    " \n",
    "# defining blue color in hsv format\n",
    "import numpy as np\n",
    " \n",
    "blueLower = np.array([100,60,60])\n",
    "blueUpper = np.array([140,255,255])\n",
    " \n",
    "kernel = np.ones((5,5), np.uint8)\n",
    " \n",
    "# define blackboard\n",
    "blackboard = np.zeros((480,640, 3), dtype=np.uint8)\n",
    "alphabet = np.zeros((200,200,3), dtype=np.uint8)\n",
    " \n",
    "# deques (Double ended queue) is used to store alphabet drawn on screen\n",
    "from collections import deque\n",
    "points = deque(maxlen = 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d8a895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### open the camera and recognize alphabet\n",
    " \n",
    "import cv2 #pip install opencv-python\n",
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "while True:\n",
    "    ret, frame=cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "     \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "     \n",
    "    # Detecting which pixel value falls under blue color boundaries\n",
    "    blue = cv2.inRange(hsv, blueLower, blueUpper)\n",
    "     \n",
    "    #erosion\n",
    "    blue = cv2.erode(blue, kernel)\n",
    "    #opening\n",
    "    blue = cv2.morphologyEx(blue, cv2.MORPH_OPEN, kernel)\n",
    "    #dilation\n",
    "    blue = cv2.dilate(blue, kernel)\n",
    "     \n",
    "    # find countours in the image\n",
    "    cnts , _ = cv2.findContours(blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "      \n",
    "    center = None\n",
    "     \n",
    "    # if any countours were found\n",
    "    if len(cnts) > 0:\n",
    "        cnt = sorted(cnts, key = cv2.contourArea, reverse=True)[0]\n",
    "        ((x,y), radius) = cv2.minEnclosingCircle(cnt)\n",
    "        cv2.circle(frame, (int(x), int(y),), int(radius), (125,344,278), 2)\n",
    "         \n",
    "        M = cv2.moments(cnt)\n",
    "        center = (int(M['m10']/M['m00']), int(M['m01']/M['m00']))\n",
    "     \n",
    "        points.appendleft(center)\n",
    "         \n",
    "    elif len(cnts) == 0:\n",
    "        if len(points) != 0:\n",
    "            blackboard_gray = cv2.cvtColor(blackboard, cv2.COLOR_BGR2GRAY)\n",
    "            blur = cv2.medianBlur(blackboard_gray, 15)\n",
    "            blur = cv2.GaussianBlur(blur, (5,5), 0)\n",
    "            thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "            #cv2.imshow(\"Thresh\", thresh)\n",
    "             \n",
    "     \n",
    "    cv2.imshow(\"Alphabet Recognition System\", frame)\n",
    "     \n",
    "    if cv2.waitKey(1)==13: #if I press enter\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 #pip install opencv-python\n",
    "cap = cv2.VideoCapture(0)\n",
    "prediction = 26\n",
    "while True:\n",
    "    ret, frame=cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "     \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "     \n",
    "    # Detecting which pixel value falls under blue color boundaries\n",
    "    blue = cv2.inRange(hsv, blueLower, blueUpper)\n",
    "     \n",
    "    #erosion\n",
    "    blue = cv2.erode(blue, kernel)\n",
    "    #opening\n",
    "    blue = cv2.morphologyEx(blue, cv2.MORPH_OPEN, kernel)\n",
    "    #dilation\n",
    "    blue = cv2.dilate(blue, kernel)\n",
    "     \n",
    "    # find countours in the image\n",
    "    cnts , _ = cv2.findContours(blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "      \n",
    "    center = None\n",
    "     \n",
    "    # if any countours were found\n",
    "    if len(cnts) > 0:\n",
    "        cnt = sorted(cnts, key = cv2.contourArea, reverse=True)[0]\n",
    "        ((x,y), radius) = cv2.minEnclosingCircle(cnt)\n",
    "        cv2.circle(frame, (int(x), int(y),), int(radius), (125,344,278), 2)\n",
    "         \n",
    "        M = cv2.moments(cnt)\n",
    "        center = (int(M['m10']/M['m00']), int(M['m01']/M['m00']))\n",
    "     \n",
    "        points.appendleft(center)\n",
    "         \n",
    "    elif len(cnts) == 0:\n",
    "        if len(points) != 0:\n",
    "            blackboard_gray = cv2.cvtColor(blackboard, cv2.COLOR_BGR2GRAY)\n",
    "            blur = cv2.medianBlur(blackboard_gray, 15)\n",
    "            blur = cv2.GaussianBlur(blur, (5,5), 0)\n",
    "            thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[1]\n",
    "            #cv2.imshow(\"Thresh\", thresh)\n",
    "             \n",
    "            blackboard_cnts = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)[0]\n",
    "             \n",
    "            if len(blackboard_cnts)>=1:\n",
    "                cnt = sorted(blackboard_cnts, key=cv2.contourArea, reverse=True)[0]\n",
    "                 \n",
    "                if cv2.contourArea(cnt)>1000:\n",
    "                    x,y,w,h = cv2.boundingRect(cnt)\n",
    "                    alphabet = blackboard_gray[y-10:y+h+10,x-10:x+w+10]\n",
    "                    try:\n",
    "                        img = cv2.resize(alphabet, (28,28))\n",
    "                    except cv2.error as e:\n",
    "                        continue\n",
    "                     \n",
    "                    img = np.array(img)\n",
    "                    img = img.astype('float32')/255\n",
    "                     \n",
    "                    prediction = model.predict(img.reshape(1,28,28))[0]\n",
    "                    prediction = np.argmax(prediction)\n",
    "                     \n",
    "            # Empty the point deque and also blackboard\n",
    "            points = deque(maxlen=512)\n",
    "            blackboard = np.zeros((480,640, 3), dtype=np.uint8)\n",
    "         \n",
    "    # connect the detected points with line\n",
    "    for i in range(1, len(points)):\n",
    "        if points[i-1] is None or points[i] is None:\n",
    "            continue\n",
    "        cv2.line(frame, points[i-1], points[i], (0,0,0), 2)\n",
    "        cv2.line(blackboard, points[i-1], points[i], (255,255,255), 8)\n",
    "         \n",
    "     \n",
    "    cv2.putText(frame, \"Prediction: \" + str(letters[int(prediction)]), (20,400), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "     \n",
    "     \n",
    "    cv2.imshow(\"Alphabet Recognition System\", frame)\n",
    "     \n",
    "    if cv2.waitKey(1)==13: #if I press enter\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa7948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
